{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Mining\n",
    "\n",
    "StyleGAN3’s latent space (Z) is unstructured. We automated a mining process using OpenAI CLIP to map it.\n",
    "\n",
    "CLIP prompts:\n",
    "1. \"intense fiery red and orange colors, aggressive sharp jagged shapes, high energy dynamic movement\"\n",
    "2. \"peaceful soft blue and teal colors, calm static atmosphere, smooth blurry gradients\"\n",
    "3. \"vibrant multicolored geometric lines, energetic neon patterns, sharp contrast\"   \n",
    "\n",
    "The generated dataset corresponds to [vettore_latente,score_prompt_1,score_prompt_2,score_prompt_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import clip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FIX PATH: Add root directory to system path to allow imports from src\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "root_path = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.insert(0, root_path)\n",
    "\n",
    "from src.gan_manager import GANManager\n",
    "\n",
    "def run_miner():\n",
    "    \"\"\"\n",
    "    Executes the Latent Space Mining process.\n",
    "    \n",
    "    This function generates random images using the GAN, analyzes them using \n",
    "    CLIP (Contrastive Language-Image Pre-Training) against specific text prompts, \n",
    "    and saves a dataset mapping latent vectors 'z' to their aesthetic scores.\n",
    "    \"\"\"\n",
    "    MODEL_PATH = './resources/network-snapshot-000280.pkl'\n",
    "    OUTPUT_PATH = './data/dataset_clip.pkl'\n",
    "    NUM_SAMPLES = 5000 \n",
    "    \n",
    "    # DEFINITION OF 3 CHROMATIC MOODS (Text Prompts)\n",
    "    PROMPTS = [\n",
    "        \"intense fiery red and orange colors, aggressive sharp jagged shapes, high energy dynamic movement\", # RED/INTENSE\n",
    "        \"peaceful soft blue and teal colors, calm static atmosphere, smooth blurry gradients\",          # BLUE/CALM\n",
    "        \"vibrant multicolored geometric lines, energetic neon patterns, sharp contrast\"                 # INTERMEDIATE/LINES\n",
    "    ]\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[*] Miner started on: {device}\")\n",
    "\n",
    "    # Load CLIP model\n",
    "    model_clip, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    text_tokens = clip.tokenize(PROMPTS).to(device)\n",
    "    \n",
    "    # Initialize GAN\n",
    "    gan = GANManager(MODEL_PATH, use_gpu=True)\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    print(f\"[*] Analyzing {NUM_SAMPLES} images...\")\n",
    "    \n",
    "    for i in tqdm(range(NUM_SAMPLES)):\n",
    "        # Generate a random latent vector z\n",
    "        z = torch.randn(1, gan.latent_dim).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate image from the GAN\n",
    "            # Note: Ensure gan.generate_image is using the correct input (z vs seed)\n",
    "            img_np = gan.generate_image(np.zeros(1024), None) \n",
    "            img_pil = Image.fromarray(img_np)\n",
    "            \n",
    "            # Preprocess for CLIP\n",
    "            image_input = preprocess(img_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Calculate similarity scores (logits) between image and text prompts\n",
    "            logits_per_image, _ = model_clip(image_input, text_tokens)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n",
    "            \n",
    "            # Append latent vector and corresponding scores to dataset\n",
    "            dataset.append({'z': z.cpu().numpy(), 'scores': probs})\n",
    "\n",
    "    # Save the dataset\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    with open(OUTPUT_PATH, 'wb') as f:\n",
    "        pickle.dump({'prompts': PROMPTS, 'data': dataset}, f)\n",
    "    \n",
    "    print(f\"[✔] Mining completed on 3 classes! Saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP TRAINING\n",
    "\n",
    "A custom Multi-Layer Perceptron (MLP) is used to act as a translator between audio features and visual moods.<br>\n",
    "The training is unsupervised using the CLIP dataset as Ground Truth.\n",
    "<br><br>\n",
    "Empirically observed min/max values for audio features to simulate realistic inputs.\n",
    "```python\n",
    "RANGES = {\n",
    "    'spectral_contrast':   {'min': (10.0, 13.03), 'med': (12.0, 14.19), 'max': (13.8, 15.33)},\n",
    "    'spectral_flatness':   {'min': (0.0012, 0.0041), 'med': (0.0028, 0.0090), 'max': (0.0049, 0.0154)},\n",
    "    'onset_strength':      {'min': (0.207, 0.502), 'med': (0.404, 0.738), 'max': (0.632, 0.869)},\n",
    "    'zero_crossing_rate':  {'min': (0.0051, 0.0143), 'med': (0.0173, 0.0455), 'max': (0.0375, 0.0666)},\n",
    "    'chroma_variance':     {'min': (0.0045, 0.0166), 'med': (0.0095, 0.0221), 'max': (0.0126, 0.0267)}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ARCHITECTURE (Synchronized with mlp_manager.py) ---\n",
    "class MoodMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) to map audio features to GAN latent vectors (w/z).\n",
    "    \n",
    "    Args:\n",
    "        input_size (int): Number of audio features (default: 5).\n",
    "        output_size (int): Dimension of the GAN latent space (default: 512).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=5, output_size=512):\n",
    "        super(MoodMLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 64), nn.ReLU(), nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 128), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.network(x)\n",
    "\n",
    "# --- REAL AUDIO FEATURE RANGES ---\n",
    "# Empirically observed min/max values for audio features to simulate realistic inputs.\n",
    "RANGES = {\n",
    "    'spectral_contrast':   {'min': (10.0, 13.03), 'med': (12.0, 14.19), 'max': (13.8, 15.33)},\n",
    "    'spectral_flatness':   {'min': (0.0012, 0.0041), 'med': (0.0028, 0.0090), 'max': (0.0049, 0.0154)},\n",
    "    'onset_strength':      {'min': (0.207, 0.502), 'med': (0.404, 0.738), 'max': (0.632, 0.869)},\n",
    "    'zero_crossing_rate':  {'min': (0.0051, 0.0143), 'med': (0.0173, 0.0455), 'max': (0.0375, 0.0666)},\n",
    "    'chroma_variance':     {'min': (0.0045, 0.0166), 'med': (0.0095, 0.0221), 'max': (0.0126, 0.0267)}\n",
    "}\n",
    "\n",
    "def get_feat(name, level):\n",
    "    \"\"\"Returns a random float within the specified feature range level.\"\"\"\n",
    "    low, high = RANGES[name][level]\n",
    "    return random.uniform(low, high)\n",
    "\n",
    "def train_mlp():\n",
    "    \"\"\"\n",
    "    Trains the MLP to associate specific audio characteristics with visual moods.\n",
    "    \n",
    "    The logic maps:\n",
    "    1. High Energy Audio -> Red/Aggressive Visuals\n",
    "    2. Low Energy Audio  -> Blue/Calm Visuals\n",
    "    3. Medium Energy     -> Vibrant/Geometric Visuals\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the CLIP-mined dataset containing (latent_vector, scores)\n",
    "    with open('./data/dataset_clip.pkl', 'rb') as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    \n",
    "    samples = raw_data['data']\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Select the top K images for each of the 3 mood categories\n",
    "    K = 1000 \n",
    "    print(f\"[*] Creating balanced dataset (Top-{K} for each of the 3 moods)...\")\n",
    "    \n",
    "    for i in range(len(raw_data['prompts'])):\n",
    "        # Extract scores for the current prompt category across all samples\n",
    "        category_scores = [s['scores'][i] for s in samples]\n",
    "        \n",
    "        # Get indices of the top K highest scoring images\n",
    "        top_indices = np.argsort(category_scores)[-K:]\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            z = samples[idx]['z'].flatten()\n",
    "            \n",
    "            # Robust Data Augmentation: Create 30 variations of audio features per image\n",
    "            for _ in range(30): \n",
    "                if i == 0: # MOOD 1: RED / AGGRESSIVE\n",
    "                    # Associates visuals with High Intensity audio features\n",
    "                    feat = [get_feat('spectral_contrast', 'max'), get_feat('spectral_flatness', 'max'), \n",
    "                            get_feat('onset_strength', 'max'), get_feat('zero_crossing_rate', 'max'), \n",
    "                            get_feat('chroma_variance', 'med')]\n",
    "                            \n",
    "                elif i == 1: # MOOD 2: BLUE / CALM\n",
    "                    # Associates visuals with Low Intensity audio features\n",
    "                    feat = [get_feat('spectral_contrast', 'min'), get_feat('spectral_flatness', 'min'), \n",
    "                            get_feat('onset_strength', 'min'), get_feat('zero_crossing_rate', 'min'), \n",
    "                            get_feat('chroma_variance', 'min')]\n",
    "                            \n",
    "                else: # MOOD 3: VIBRANT / GEOMETRIC\n",
    "                    # Associates visuals with Medium Intensity (but High Chroma)\n",
    "                    feat = [get_feat('spectral_contrast', 'med'), get_feat('spectral_flatness', 'med'), \n",
    "                            get_feat('onset_strength', 'med'), get_feat('zero_crossing_rate', 'med'), \n",
    "                            get_feat('chroma_variance', 'max')]\n",
    "                \n",
    "                X.append(feat)\n",
    "                Y.append(z)\n",
    "\n",
    "    # Convert to NumPy arrays and normalize features\n",
    "    X_np = np.array(X, dtype=np.float32)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_np)\n",
    "    Y_np = np.array(Y, dtype=np.float32)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    loader = DataLoader(TensorDataset(torch.FloatTensor(X_scaled), torch.FloatTensor(Y_np)), batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Initialize Model and Optimizer\n",
    "    model = MoodMLP().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"[*] Training (300 epochs)...\")\n",
    "    for epoch in tqdm(range(300)):\n",
    "        for bx, by in loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(bx)\n",
    "            loss = criterion(prediction, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Save Model and Scaler\n",
    "    model.to('cpu')\n",
    "    torch.save(model.state_dict(), './resources/mood_mlp.pth')\n",
    "    \n",
    "    with open('./resources/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    print(\"[✔] Model saved! The GAN will now react to the 3 audio moods.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
